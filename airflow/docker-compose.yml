
services: 

    postgres:
      image: postgres:13
      container_name: postgres_container
      restart: always
      environment:
        POSTGRES_USER: airflow
        POSTGRES_PASSWORD: airflow
        POSTGRES_DB: airflow
      ports:
        - "5432:5432"
      volumes:
        - postgres-db-volume:/var/lib/postgresql/data

    redis:
        image: redis:latest
        expose:
          - 6379
        healthcheck:
          test: ["CMD", "redis-cli", "ping"]
          interval: 5s
          timeout: 30s
          retries: 50
        restart: always    

    airflow:       
        # # privileged: true
        # user: "root"      
        # image: apache/airflow:2.8.1-python3.8
        image: real-time-airflow-image                    
        build: './airflow_docker'                       
        container_name: airflow_container
        entrypoint: /opt/airflow/entrypoint.sh  
        restart: always
        depends_on:
          - postgres
        environment:
            AIRFLOW_HOME: /opt/airflow
            AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres_container:5432/airflow
            AIRFLOW__CORE__EXECUTOR: LocalExecutor
            AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'True'
            AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
            AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
            AIRFLOW__CORE__FERNET_KEY: 'ffHBWrWkkmkrpqEu_9fz8-sF0SdI-OAvUQ1OeQNoomE='
            AIRFLOW__CORE__PARALLELISM: 16     # Total running tasks
            AIRFLOW__CORE__DAG_CONCURRENCY: 8  # Number of tasks per DAG run
            AIRFLOW__SCHEDULER__SCHEDULER_MAX_THREADS: 4
            AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: 600
            dags_folder: /opt/airflow/dags
            WEB_SERVER_PORT: 3000
            NLTK_DATA: /opt/airflow/nltk_data
            PYTHONPATH: /home/airflow/.local/lib/python3.8/site-packages:$PYTHONPATH:/opt/airflow/plugins
            YFINANCE_CACHE_DIR: /opt/airflow/.cache/py-yfinance

            PYSPARK_SUBMIT_ARGS: "--packages org.postgresql:postgresql:42.2.20"
            PYSPARK_PYTHON: /usr/local/bin/python
        volumes:                                        
            - ./dags:/opt/airflow/dags:rw
            - ./data:/opt/airflow/data:rw
            - ./plugins:/opt/airflow/plugins:rw
            - ./nltk_data:/opt/airflow/nltk_data
            - ./airflow_docker/airflow_env.sh:/opt/airflow/airflow_env.sh:ro
            - ./airflow_docker/entrypoint.sh:/opt/airflow/entrypoint.sh:ro
            - ./spark-logs:/opt/spark-logs:rw
            - ./api:/opt/airflow/api:rw
            - /data/seanchoi:/data/seanchoi:ro

        ports:
            - "8080:8080"   # Airflow Webserver
            - "3000:3000"   # Custom Webserver Port
            - "4040:4040"   # Spark UI
        command: webserver
        healthcheck:
          test: ["CMD", "curl", "--fail", "http://localhost:3000/health"]
          interval: 10s
          timeout: 10s
          retries: 5
        deploy:
          resources:
            limits:
              cpus: '3.0'       # allow up to 3 CPUs
              memory: 12G       # allow up to 12 GB RAM
            reservations:
              cpus: '2.0'       # reserve 2 CPUs minimum
              memory: 6G        # reserve 6 GB minimum


    pdcmmetastore:
        image: postgres:14
        container_name: pdcmmetastore_container
        environment:
          POSTGRES_USER: pdcm
          POSTGRES_PASSWORD: pdcm
          POSTGRES_DB: pdcm
        volumes:
          - pdcmmetastore-db-volume:/var/lib/postgresql/data
        healthcheck:
          test: ["CMD", "pg_isready", "-U", "pdcm"]
          interval: 5s
          retries: 5
        restart: always
        ports:
          - "5433:5432"  # PDCMmetastore Postgres port
      


volumes:
  pdcmmetastore-db-volume:
  postgres-db-volume:
